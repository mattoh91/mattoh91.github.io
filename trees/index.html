<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://mattoh91.github.io/ml-notes/trees/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Tree-Based Models - Matt's ML Notes</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Tree-Based Models";
        var mkdocs_page_input_path = "trees.md";
        var mkdocs_page_url = "/ml-notes/trees/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Matt's ML Notes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Welcome to Matt's AI Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../hyperparam/">Hyperparameter Tuning</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Tree-Based Models</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#decision-trees">Decision Trees</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#classifier-trees">Classifier Trees</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#regression-trees">Regression Trees</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pruning">Pruning</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#random-forests">Random Forests</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#adaboost">AdaBoost</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#gradient-boost">Gradient Boost</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgboost">XGBoost</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a>
    </li>
    </ul>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Matt's ML Notes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>Tree-Based Models</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="tree-based-models">Tree-Based Models</h1>
<h2 id="decision-trees">Decision Trees</h2>
<p>The algorithm iteratively goes through each feature and finds the best split for each depth based on a splitting criterion.</p>
<h3 id="classifier-trees">Classifier Trees</h3>
<ul>
<li>Splitting criterion</li>
<li>
<p>Gini Impurity<br />
  For J classes where a i ∈ {1, ..., J}:</p>
<p><img alt="gini impurity formula" src="../img/gini_formula.png" /></p>
</li>
<li>
<p>Entropy (Information Gain)</p>
</li>
<li>
<p>Categorical feature splits
This can be done in 1 of 2 ways:</p>
</li>
<li>One-hot encoding: Every level has its own binary column. For a level <code>x</code>, a node will be split into child nodes of column <code>x</code> = 1 or 0.</li>
<li>Ordinal encoding: There will be a single numerical column of integers representing the different levels. Considering how the input data matrix would not be sparse as with one-hot encoding, this may be computationally more efficient with the caveat that computing 1's and 0's are easier. Refer to how numeric splits work below.</li>
<li>Numeric feature splits:</li>
<li>Numeric column is first sorted from smallest to largest.</li>
<li>Average is computed between each successive row.</li>
<li>
<p>Each average is used to split the node and compute the split criterion metric.</p>
<p><img alt="classification tree numeric splits" src="../img/classtree_numsplit.png" /></p>
</li>
<li>
<p>The average with the best split criterion metric is chosen.</p>
<p><img alt="classification tree numeric split eg" src="../img/classtree_numsplit_eg.png" /></p>
</li>
<li>
<p>Prediction: If a leaf consists multiple samples, the predicted class is the mode.</p>
</li>
</ul>
<h3 id="regression-trees">Regression Trees</h3>
<ul>
<li>
<p>Splitting criterion: Sum of squared errors (SSR)</p>
</li>
<li>
<p>Numeric feature splits:</p>
</li>
</ul>
<p><img alt="regression tree numeric split" src="../img/regtree_numsplit.png" /></p>
<ul>
<li>Iterate through each sample and use the average between the current and next sample as a threshold to split the dataset into 2 groups.</li>
<li>Compute the combined SSR for both groups during each iteration.</li>
<li>
<p>The average that results in the best split (lowest SSR) will be used to split the node.</p>
</li>
<li>
<p>Categorical feature splits: Similar method to numeric feature splits is employed except that instead of the average between each sample being iteratively used as a threshold to split the dataset for SSR computation, the split used for SSR computation is based on whether samples have a categorical value or not.</p>
</li>
</ul>
<p><img alt="regression tree categorical split" src="../img/regtree_catsplit.png" /></p>
<ul>
<li>Prediction: If a leaf consists multiple samples, their average target value is taken to represent that leaf.</li>
</ul>
<h3 id="pruning">Pruning</h3>
<ul>
<li>Pre-pruning</li>
<li>Post-pruning</li>
</ul>
<h2 id="random-forests">Random Forests</h2>
<h2 id="adaboost">AdaBoost</h2>
<p>The core principle of AdaBoost is to fit a sequence of weak learners on repeatedly modified versions of the data. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Training samples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, samples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the samples that are missed by the previous ones in the sequence. The key steps have been documented below:</p>
<ul>
<li>Initialise a sample weights column which indicates how important it is for a sample to be correctly classified. This is done by taking <code>1/m</code> where m is the number of samples in the dataset - total weights should sum to 1.</li>
</ul>
<p><img alt="ada init sample weights" src="../img/ada_initsampleweights.png" /></p>
<ul>
<li>Build a stump with a relevant split criterion - Gini / Entropy for classification; SSR for regression.</li>
<li>The stump only has 1 root node that splits into 2 leaves immediately.</li>
<li>This is done using the feature that gives the best split criterion value.</li>
<li>Compute <code>amount of say</code> (output weight) for the stump using <code>total error</code> [0,1], which is the sum of weights of the incorrectly classified samples:</li>
</ul>
<p><img alt="ada amount of say formula" src="../img/ada_amtsay_formula.png" /><br />
<img alt="ada amount of say graph" src="../img/ada_amtsay_graph.png" /></p>
<ul>
<li>
<p>If stump total error is high, <code>amount of say</code> is negative - will reverse its output.</p>
</li>
<li>
<p>Update weight samples such that incorrectly classified samples have greater weight values.</p>
</li>
</ul>
<p><img alt="ada weight incorrect update formula" src="../img/ada_incorrect_weightupdate_formula.png" /><br />
<img alt="ada weight incorrect update graph" src="../img/ada_incorrect_weightupdate_graph.png" /></p>
<ul>
<li>Update weight samples such that correctly classified samples have smaller weight values.</li>
</ul>
<p><img alt="ada weight correct update formula" src="../img/ada_correct_weightupdate_formula.png" /><br />
<img alt="ada weight correct update graph" src="../img/ada_correct_weightupdate_graph.png" /></p>
<ul>
<li>Normalise the updated weights so they sum to 1.</li>
</ul>
<p><img alt="ada normalise weights" src="../img/ada_normweights.png" /></p>
<ul>
<li>Create the next stump either by:</li>
<li>Using the normalised weights to compute the weighted split criterion per feature.</li>
<li>
<p>Using the normalised weights as a distribution to draw samples from the original dataset.</p>
<ul>
<li>As incorrectly classified samples have large weights, they take up a larger range in the distribution and have a higher chance of being sampled.</li>
<li>Sample until you get the same amount of rows as the original dataset.</li>
<li>Lastly, reinitialise the sample weights as <code>1/m</code>. The incorrectly classified samples need not retain their higher weight values as they should have been repeatedly sampled into duplicate rows.</li>
</ul>
</li>
<li>
<p>Prediction:</p>
</li>
<li>Classification: The class predicted by the stumps with the highest sum of <code>amount of say</code> will be the final prediction</li>
<li>Regression: The predictions from all stumps are then combined through a weighted sum to produce the final prediction (sklearn).</li>
</ul>
<h2 id="gradient-boost">Gradient Boost</h2>
<ul>
<li>Make initial prediction:</li>
<li>Regression: Average target value</li>
<li>Classification: Initial prediction is a <code>log odds</code> where <code>odds</code> equal to <code>class count / not class count</code>.</li>
<li>Weak learners are trees:</li>
<li>That are split based on the standard decision tree splitting criterion.</li>
<li>With <code>8 - 32</code> leaves.</li>
<li>Scaled by the same learning rate.</li>
<li>Trained to predict the pseudo residual.</li>
<li>The pseudo residual:</li>
<li>
<p>To compute the pseudo residual, the inital prediction value needs to be converted from a <code>log odds</code> into a <code>probability</code> using a logistic function:</p>
<p><img alt="log odds to prob" src="../img/logoddstoprob.png" /></p>
</li>
<li>
<p>The residuals are then transformed back into a <code>log odds</code> output using this formula so that they can be added to the initial prediction which is a <code>log odds</code>:</p>
<p><img alt="prob to log odds" src="../img/gradboost_probtologodds.png" /></p>
</li>
<li>
<p>Can also be mathematically expressed as the function of the partial derivative / gradient of the loss function against the predicted value, hence the "Gradient" in "Gradient Boost":
    <img alt="gradient origins" src="../img/gradient_origins.png" /></p>
</li>
<li>Keep building more trees until </li>
<li>Additional trees do not significantly reduce size of residuals.</li>
<li>Maximum number of trees built.</li>
<li>Prediction: </li>
<li>For regression, this is done by adding scaled (by learning rate) predicted residuals of each learner to the initial prediction.</li>
<li>
<p>For classification:</p>
<ul>
<li>Residuals have been transformed to a <code>log odds</code> output.</li>
<li>This output is added to the initial prediction to gve a <code>log odds</code> prediction.</li>
<li>An additional transformation needs to be applied to <code>log odds</code> to convert it into a <code>probability</code> using a logistic function:</li>
</ul>
<p><img alt="logodds to prob" src="../img/logoddstoprob.png" /></p>
</li>
</ul>
<h2 id="xgboost">XGBoost</h2>
<ul>
<li>Make initial prediction with value of <code>0.5</code> whether XGBoost is used for regression or classification.</li>
<li>For classification, this is treated to be a <code>probability</code>. This is to be distinguished from standard Gradient Boost where the initial prediction is a <code>log odds</code>.</li>
<li>Weak learners:</li>
<li>Starts off as a root node of residuals (target - 0.5).</li>
<li>Are <code>NOT</code> decision trees as they are split based on the <code>gain in a similarity score</code> (not split criterion such as Gini, Entropy, SSR) when comparing a parent node with its potential child nodes. This distinguishes XGBoost learners from Gradient Boost learners.</li>
<li>The similarity score is a function of each sample's feature 'residual': eg. row 1 has col 1 value of 123 which gives residual of 23 from average prediction of 100.</li>
<li>
<p>Similarity score for regression:</p>
<p><img alt="XG reg sim score" src="../img/xg_reg_simscore.png" /></p>
</li>
<li>
<p>Similarity score for classification which transforms residuals from a <code>probability</code> into a <code>log odds</code>. Its numerator is the same as the similarity score formula for regression.:</p>
<p><img alt="XG class sim score" src="../img/xg_class_simscore.png" /></p>
</li>
<li>
<p>The similarity gain is the difference between the similarity score of the parent and its potential child nodes.</p>
</li>
<li>This is unlike standard Gradient Boost which grows its learners using a standard decision tree's splitting criterion.</li>
<li>Prediction: </li>
<li>
<p>Regression:</p>
<ul>
<li>First uses the following output formula to compute the value of each leaf. This is similar to the similarity score except that the sum of residuals numerator is not squared:</li>
</ul>
<p><img alt="XG reg output formula" src="../img/xg_reg_output.png" /></p>
<ul>
<li>The output scores are multiplied with the learning rate η (eta) - with default value of <code>0.3</code> - and added to the initial predicted value:</li>
</ul>
<p><img alt="XG pred" src="../img/xg_pred.png" /></p>
<ul>
<li>The new residuals are computed by taking <code>target - (init pred + η * output)</code>. These residuals are then used to build the next tree.</li>
<li>Classification:</li>
<li>Similar to regression, the residuals are used to compute output values, albeit with a different formula.</li>
</ul>
<p><img alt="XG class output formula" src="../img/xg_class_output.png" /></p>
<ul>
<li>Again, the output formula is the same as the similarity score formula except that its numerator is not squared.</li>
<li>The XGBoost output formula is the same as what was used in standard Gradient Boost to convert residuals into <code>log odds</code> except for the addition of Lambda in the denominator.</li>
<li>To compute the new residuals, the initial prediction needs to be converted from a <code>probability</code> into a <code>log odds</code> using the formula below:</li>
</ul>
<p><img alt="prob to logodds" src="../img/probtologodds.png" /></p>
<ul>
<li>The log odds predicted value is done using <code>init pred + η * output</code>. It is then converted into a probability, <code>pred proba</code>.</li>
<li>The new residuals are computed by taking <code>target - pred proba</code>, where <code>target</code> is 1 or 0. These residuals are then used to build the next tree.</li>
<li>Keep building more trees until </li>
<li>Additional trees do not significantly reduce size of residuals.</li>
<li>Maximum number of trees built.</li>
<li>Hyperparameters:</li>
<li>λ (Lambda): L2 regularisation term</li>
<li>Reduces prediction sensitivity to inidivual observations. More </li>
<li>If Lambda &gt; 0, similarity gain values are smaller. In conjunction with Gamma, pruning is more aggressive.</li>
<li>α (Alpha): L1 regularisation term</li>
<li>γ (Gamma): Minimum similarity score gain required to make a further partition on a leaf node of the tree. </li>
<li>Cover: Number of residuals in each leaf. This is equivalent to the denominator of the similarity score less Lambda.</li>
</ul>
</li>
</ul>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Learner</th>
<th>Splits</th>
<th>Bootstrap</th>
<th>Learner Weights</th>
<th>Predicts</th>
<th>Ensemble Method</th>
<th>Bias / Variance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Forest</td>
<td>Full trees</td>
<td>Same as DT</td>
<td>Random</td>
<td>Equal</td>
<td>Output</td>
<td>Bagging</td>
<td>High bias</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>Stumps</td>
<td>Same as DT</td>
<td>None or weight-based</td>
<td>Weighted</td>
<td>Output</td>
<td>Boosting</td>
<td>High variance</td>
</tr>
<tr>
<td>Gradient Boost</td>
<td>Trees (8 - 32 leaves)</td>
<td>Same as DT</td>
<td>None</td>
<td>Equal using learning rate</td>
<td>Error / Pseudo residual</td>
<td>Gradient Boosting</td>
<td>High variance</td>
</tr>
<tr>
<td>XGBoost</td>
<td>"XGBoost" Trees (depth of 6)</td>
<td>Similarity Score Gain</td>
<td>None</td>
<td>Equal using learning rate</td>
<td>Error / Pseudo residual</td>
<td>XGradient Boosting</td>
<td>High variance</td>
</tr>
</tbody>
</table>
<ul>
<li>Another important distinction between Gradient Boost and XGBoost is that their classififiers have different types of initial prediction values.</li>
<li>Gradient Boost: Log odds</li>
<li>XGBoost: Probability</li>
</ul>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Initial Prediction</th>
<th>Residual</th>
<th>Learner Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Boost</td>
<td>Log odds</td>
<td>Proba -&gt; Log odds</td>
<td>Log odds -&gt; Proba</td>
</tr>
<tr>
<td>XG Boost</td>
<td>Proba -&gt; Log odds</td>
<td>Proba -&gt; Log odds</td>
<td>Log odds -&gt; Proba</td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../hyperparam/" class="btn btn-neutral float-left" title="Hyperparameter Tuning"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../hyperparam/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
