<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://mattoh91.github.io/ml-notes/nlp/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>NLP - Matt's ML Notes</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "NLP";
        var mkdocs_page_input_path = "nlp.md";
        var mkdocs_page_url = "/ml-notes/nlp/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Matt's ML Notes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Welcome to Matt's AI Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../attention/">Attention</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../hyperparam/">Hyperparameter Tuning</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../llms/">BERT</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">NLP</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../regularisation/">Regularisation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../rl/">Reinforcement Learning</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../transformers/">Transformers</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../trees/">Tree-Based Models</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Matt's ML Notes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>NLP</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="nlp">NLP</h1>
<p>At the core of natural language processing is the representation of words, phrases and sentences as machine-readable input.<br />
The list below details the evolutionary path of this representation:
1. Non-contextual embeddings
  * Bag of words (BoW): A matrix of vectors that are numeric representations of tokenized text input. Specifically this BoW numerically represents the frequency of occurrence of words in a corpus.
  * NN-generated embeddings (eg. Word2Vec / CBOW): Provides no context - only gives 1 representation of a word. Analogous to a lookup table, in stark contrast to contextual embeddings (eg. from BERT) that is able to output different embeddings for the same word used in different input sequences.
2. Seq2Seq: Architecture comprising 2 RNNs - an encoder and a decoder - to perform natural language translation. This is used over a single RNN which would limit the output sequence to be the same size as the input sequence.
3. Attention:  Used in the context of neural machine language translation, addresses the problem of the Seq2Seq RNN architecture having an insufficiently large context vector to represent the entirety of the input sequence.
4. Self-attention: Used in the context of GLMs to create a representation of words in an input sequence. Apart from context, another difference between self-attention and attention is that self-attention considers context relative to "self" - the input sequence words - whilst attention considers context relative to the (hidden states of) decoder's output sequence. Self-attention is also faster as it processes the entirety of a sequence as input as opposed to one token at a time. Lastly, self-attention is also not bogged down by any coupling to a decoder output sequence.
Self-attention trains weights for q,k,v that are optimized to capture a context for each word in an input sequence. An analogy would be q,k,v used in a database - q is a query into a particular context; k gives rise to the relevance/similarity between words to a q; v is the word value itself to which attention score is applied to give rise to a context embedding.
5. Multi-head attention: Essentially a stacked self-attention architecture where each stack/head serves to train q,k,v weights that are optimized to capture multiple latent contexts for each word. Think of each head to learn 1 context for each word in the input sequence - in a single head, each word only has one weight matrix for q to capture 1 context</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../llms/" class="btn btn-neutral float-left" title="BERT"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../regularisation/" class="btn btn-neutral float-right" title="Regularisation">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../llms/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../regularisation/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
