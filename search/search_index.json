{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Matt's AI Notes This archive of notes was created using mkdocs. For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Welcome to Matt's AI Notes"},{"location":"#welcome-to-matts-ai-notes","text":"This archive of notes was created using mkdocs. For full documentation visit mkdocs.org .","title":"Welcome to Matt's AI Notes"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"attention/","text":"Attention Sequence-to-Sequence (Seq2Seq) models from the pioneering paper by Sutskever et al. (2014) and Cho et al. (2014b) only outputted a small context vector from their encoders to their decoders. This context vector is incapable of capturing more information due to its small size. The attention mechanism was thus born out of the endeavour to provide richer context information for downstream tasks. Eventually, attention soon evolved into the self-attention mechanism introduced by Vaswani's (2017) seminal paper, \"Attention Is All You Need\". It differs from its predecessor in the following areas: * Self-attention considers context relative to \"self\" - the input sequence words - whilst attention considers context relative to the (hidden states of) decoder's output sequence. * Self-attention is also faster as it processes the entirety of a sequence as input as opposed to one token at a time. * Lastly, self-attention is also not bogged down by any coupling to a decoder output sequence. Papers: * Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, et al., 2014) * Effective approaches to attention-based neural machine translation (Luong, et al., 2015) * Attention is all you need (Vaswani, et al., 2017) Other references: * Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) article * Illustrated attention article * Illustrated self-attention article","title":"Attention"},{"location":"attention/#attention","text":"Sequence-to-Sequence (Seq2Seq) models from the pioneering paper by Sutskever et al. (2014) and Cho et al. (2014b) only outputted a small context vector from their encoders to their decoders. This context vector is incapable of capturing more information due to its small size. The attention mechanism was thus born out of the endeavour to provide richer context information for downstream tasks. Eventually, attention soon evolved into the self-attention mechanism introduced by Vaswani's (2017) seminal paper, \"Attention Is All You Need\". It differs from its predecessor in the following areas: * Self-attention considers context relative to \"self\" - the input sequence words - whilst attention considers context relative to the (hidden states of) decoder's output sequence. * Self-attention is also faster as it processes the entirety of a sequence as input as opposed to one token at a time. * Lastly, self-attention is also not bogged down by any coupling to a decoder output sequence. Papers: * Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, et al., 2014) * Effective approaches to attention-based neural machine translation (Luong, et al., 2015) * Attention is all you need (Vaswani, et al., 2017) Other references: * Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) article * Illustrated attention article * Illustrated self-attention article","title":"Attention"},{"location":"hyperparam/","text":"Hyperparameter Tuning Nested Cross-Validation Bayesian Tree-structured Parzen Estimator (TPE)","title":"Hyperparameter Tuning"},{"location":"hyperparam/#hyperparameter-tuning","text":"","title":"Hyperparameter Tuning"},{"location":"hyperparam/#nested-cross-validation","text":"","title":"Nested Cross-Validation"},{"location":"hyperparam/#bayesian","text":"","title":"Bayesian"},{"location":"hyperparam/#tree-structured-parzen-estimator-tpe","text":"","title":"Tree-structured Parzen Estimator (TPE)"},{"location":"llms/","text":"BERT GPT GPT-1 Motivations: Mechanics: Mathematics: Paper: Improving Language Understanding by Generative Pre-Training (Radford, et al., 2018) Other references: GPT-2 Motivations: Mechanics: Mathematics: Paper: Other references: The illustrated GPT-2 article","title":"BERT"},{"location":"llms/#bert","text":"","title":"BERT"},{"location":"llms/#gpt","text":"","title":"GPT"},{"location":"llms/#gpt-1","text":"Motivations: Mechanics: Mathematics: Paper: Improving Language Understanding by Generative Pre-Training (Radford, et al., 2018) Other references:","title":"GPT-1"},{"location":"llms/#gpt-2","text":"Motivations: Mechanics: Mathematics: Paper: Other references: The illustrated GPT-2 article","title":"GPT-2"},{"location":"nlp/","text":"NLP At the core of natural language processing is the representation of words, phrases and sentences as machine-readable input. The list below details the evolutionary path of this representation: 1. Non-contextual embeddings * Bag of words (BoW): A matrix of vectors that are numeric representations of tokenized text input. Specifically this BoW numerically represents the frequency of occurrence of words in a corpus. * NN-generated embeddings (eg. Word2Vec / CBOW): Provides no context - only gives 1 representation of a word. Analogous to a lookup table, in stark contrast to contextual embeddings (eg. from BERT) that is able to output different embeddings for the same word used in different input sequences. 2. Seq2Seq: Architecture comprising 2 RNNs - an encoder and a decoder - to perform natural language translation. This is used over a single RNN which would limit the output sequence to be the same size as the input sequence. 3. Attention: Used in the context of neural machine language translation, addresses the problem of the Seq2Seq RNN architecture having an insufficiently large context vector to represent the entirety of the input sequence. 4. Self-attention: Used in the context of GLMs to create a representation of words in an input sequence. Apart from context, another difference between self-attention and attention is that self-attention considers context relative to \"self\" - the input sequence words - whilst attention considers context relative to the (hidden states of) decoder's output sequence. Self-attention is also faster as it processes the entirety of a sequence as input as opposed to one token at a time. Lastly, self-attention is also not bogged down by any coupling to a decoder output sequence. Self-attention trains weights for q,k,v that are optimized to capture a context for each word in an input sequence. An analogy would be q,k,v used in a database - q is a query into a particular context; k gives rise to the relevance/similarity between words to a q; v is the word value itself to which attention score is applied to give rise to a context embedding. 5. Multi-head attention: Essentially a stacked self-attention architecture where each stack/head serves to train q,k,v weights that are optimized to capture multiple latent contexts for each word. Think of each head to learn 1 context for each word in the input sequence - in a single head, each word only has one weight matrix for q to capture 1 context","title":"NLP"},{"location":"nlp/#nlp","text":"At the core of natural language processing is the representation of words, phrases and sentences as machine-readable input. The list below details the evolutionary path of this representation: 1. Non-contextual embeddings * Bag of words (BoW): A matrix of vectors that are numeric representations of tokenized text input. Specifically this BoW numerically represents the frequency of occurrence of words in a corpus. * NN-generated embeddings (eg. Word2Vec / CBOW): Provides no context - only gives 1 representation of a word. Analogous to a lookup table, in stark contrast to contextual embeddings (eg. from BERT) that is able to output different embeddings for the same word used in different input sequences. 2. Seq2Seq: Architecture comprising 2 RNNs - an encoder and a decoder - to perform natural language translation. This is used over a single RNN which would limit the output sequence to be the same size as the input sequence. 3. Attention: Used in the context of neural machine language translation, addresses the problem of the Seq2Seq RNN architecture having an insufficiently large context vector to represent the entirety of the input sequence. 4. Self-attention: Used in the context of GLMs to create a representation of words in an input sequence. Apart from context, another difference between self-attention and attention is that self-attention considers context relative to \"self\" - the input sequence words - whilst attention considers context relative to the (hidden states of) decoder's output sequence. Self-attention is also faster as it processes the entirety of a sequence as input as opposed to one token at a time. Lastly, self-attention is also not bogged down by any coupling to a decoder output sequence. Self-attention trains weights for q,k,v that are optimized to capture a context for each word in an input sequence. An analogy would be q,k,v used in a database - q is a query into a particular context; k gives rise to the relevance/similarity between words to a q; v is the word value itself to which attention score is applied to give rise to a context embedding. 5. Multi-head attention: Essentially a stacked self-attention architecture where each stack/head serves to train q,k,v weights that are optimized to capture multiple latent contexts for each word. Think of each head to learn 1 context for each word in the input sequence - in a single head, each word only has one weight matrix for q to capture 1 context","title":"NLP"},{"location":"regularisation/","text":"Regularisation Regularisation is a method of reducing overfitting / variance by adding a penalty term to the loss function. The penalty term is the norm of the parameter, where \"norm\" refers to vector norm , which encompasses different ways to measure the magnitude of a vector. As the loss function is used in an argmin to find the optimal model parameters, increasing the regularisation hyperparameter forces the parameters it is multiplied with to shrink. Overall, shrinking the parameters reduces the complexity of the model. L1 / Lasso Uses the L1 norm as the regularisation / penalty term in the loss function. As the L1 norm is the sum of absolute values of a vector, it is mathematically similar to a square funciton: $abs(x) + abs(y) = diag$ Thus, it imposes a linear constraint: L2 / Ridge Uses the L2 norm as the regularisation / penalty term in the loss function. As the L2 norm is the square root of the sum of squared values of a vector, it is mathematically similar to a circle function: $x^2 + y^2 = r^2$ Thus, it imposes a radial constraint: Unlike the L1 regularisation, the radial contours result in an optimal loss function with parameter values that do not lie on the axes. L2 is hence unable to shrink parameters to 0 unlike L1 regularisation. References: Visual explanation for regularisation of linear models article How does Lasso regression(L1) encourage zero coefficients but not the L2 article","title":"Regularisation"},{"location":"regularisation/#regularisation","text":"Regularisation is a method of reducing overfitting / variance by adding a penalty term to the loss function. The penalty term is the norm of the parameter, where \"norm\" refers to vector norm , which encompasses different ways to measure the magnitude of a vector. As the loss function is used in an argmin to find the optimal model parameters, increasing the regularisation hyperparameter forces the parameters it is multiplied with to shrink. Overall, shrinking the parameters reduces the complexity of the model.","title":"Regularisation"},{"location":"regularisation/#l1-lasso","text":"Uses the L1 norm as the regularisation / penalty term in the loss function. As the L1 norm is the sum of absolute values of a vector, it is mathematically similar to a square funciton: $abs(x) + abs(y) = diag$ Thus, it imposes a linear constraint:","title":"L1 / Lasso"},{"location":"regularisation/#l2-ridge","text":"Uses the L2 norm as the regularisation / penalty term in the loss function. As the L2 norm is the square root of the sum of squared values of a vector, it is mathematically similar to a circle function: $x^2 + y^2 = r^2$ Thus, it imposes a radial constraint: Unlike the L1 regularisation, the radial contours result in an optimal loss function with parameter values that do not lie on the axes. L2 is hence unable to shrink parameters to 0 unlike L1 regularisation.","title":"L2 / Ridge"},{"location":"regularisation/#references","text":"Visual explanation for regularisation of linear models article How does Lasso regression(L1) encourage zero coefficients but not the L2 article","title":"References:"},{"location":"rl/","text":"Reinforcement Learning Reinforcement Learning (RL) is a branch of Machine Learning. Unlike supervised learning where you specify what the model should be trained to output , RL involves defining a reward function to tell the model what it is doing well or poorly, and let the model figure out how to do things by choosing good actions at various states. Policy Return Return is the sum of the rewards that the system gets weighted by the discount factor $\\gamma$, where rewards in the future are are weighted by $\\gamma$ raised to a higher power. Semantically this means that the system will be disincentivised to pursue delayed rewards. Note that the concept of negative rewards also exists, where the system is incentivised to delay its pursuit of such rewards (eg. paying back a loan). Markov Decision Process (MDP) Framework where the next immediate future state only depends on the current state State Action Value Function (Q Function) This is expressed as the function $Q(s,a) = Return$ if: * You start in state $s$ * Take action $a$ (once) * Then behave optimally after that Bellman Equation This equation is used to compute the Q Function. Take note of the following notation convention: Terminal states have the following Bellman Equation since they have no \"next\" state and action: $Q(s,a) = R(s)$ The Bellman Equation can be split into 2 components: Stochastic Environment The system may not have deterministic / fixed actions. Eg. The Mars Rover could slip and make a step in the opposite direction. This warrants the introduction of randomness into the system. Return is treated as a random variable. The goal of the Bellman Equation would thus be to maximise the Expected Return , which is analogous to the average of the random variable Return . The Bellman Equation will include the expected maximum return of the next state. Continuous State Spaces The initial simplified lunar landing rover example uses discrete states where it could be in 1 of 6 different positions. Real-life RL applications could involve states that take on continuous values. For example a vehicle could require the following continuous states: X-position Y-position Orientation / Facing Speed in x-direction Speed in y-direction Turning speed These continuous states are represented using a vector. Note that this vector can also contain discrete values if needed. Deep RL Deep RL will be discussed using the example of the Lunar Lander. x and y refer to horizontal and vertical distances. $\\dot{x}$ and $\\dot{y}$ refer to their corresponding velocities. $\\theta$ refers to the lander's angle and $\\dot{\\theta}$ refers to the velocity of its tilting. $r$ and $l$ are binary states referring to the grounding of the right and left legs of the lander. Final layer with a single neuron is used to output $Q(s,a)$ which will also be referred to as $y$. $a$ is a component of vector $x$ that comprises booleans representing the 4 actions to take. Nothing = no thrusters; left = left thrusters; right = right thruster; main = main thruster General concept is to: Use Bellman's Equation to create a large dataset of multiple $\\vec{x}$ and $y$ Use supervised learning to learn a mapping from $\\vec{x}$ to $y$ The algorithm to do this is called the Deep Q Network (DQN) method: A neural network $Q$ is randomly initialised similar to what is normally done for weight matrices. This initialisation is a random guess of what the $Q$ function should be. Generate the replay buffer (10,000 training examples) by feeding random $(s,a)$ into the neural network - which outputs the $Q(s,a)$ function. Record the resulting states and rewards $(R(s),s')$. A new neural network $Q_{new}$ is trained to map current (state and action) $x$, to the Bellman Equation $y$. The $Q(s',a')$ component of the Bellman Equation is obtained by passing $x = (s',a')$ into the initial $Q from step 1 for all 4 possible values of $a'$. $Q_{new}$ replaces the old $Q$ function. Steps 1 to 4 are repeated several times. With every iteration, the neural network will become a better estimate for the $Q$ function. The DQN architecture can be improved by: Setting its inputs to solely contain the states $s$ without the actions $a$. Changing the final layer to comprise a neuron to predict each action such that all actions can be predicted simultaneously. Epsilon Greedy Policy Used to pick actions while system is still in the process of learning. Comprises a greedy exploitation component that picks an action $a$ that maximises the $Q$ function. Comprises a second $\\epsilon$ exploration hyperparameter to randomly try another action. This is to overcome the problem of potentially having a bad initialisation of $Q$ as exploration enables the system to try an action that it may have be initialised to recognise to result in a poor reward. Take note that RL is very sensitive to hyperparameters, which makes optimisation more difficult than supervised learning. DQN Mini-Batch and Soft Updates Mini-batching results in a more efficient optmisation step. For example SGD using all samples would take more time as it requires computing the gradient of the loss function which is an average. However this may abruptly compromise the update of $Q$ to $Q_{new}$ if the mini-batch happens to produce a bad update Soft update addresses this problem by assigning hyperparameterised coefficients to weight the amount of update to the old neural network weights and biases.","title":"Reinforcement Learning"},{"location":"rl/#reinforcement-learning","text":"Reinforcement Learning (RL) is a branch of Machine Learning. Unlike supervised learning where you specify what the model should be trained to output , RL involves defining a reward function to tell the model what it is doing well or poorly, and let the model figure out how to do things by choosing good actions at various states.","title":"Reinforcement Learning"},{"location":"rl/#policy","text":"","title":"Policy"},{"location":"rl/#return","text":"Return is the sum of the rewards that the system gets weighted by the discount factor $\\gamma$, where rewards in the future are are weighted by $\\gamma$ raised to a higher power. Semantically this means that the system will be disincentivised to pursue delayed rewards. Note that the concept of negative rewards also exists, where the system is incentivised to delay its pursuit of such rewards (eg. paying back a loan).","title":"Return"},{"location":"rl/#markov-decision-process-mdp","text":"Framework where the next immediate future state only depends on the current state","title":"Markov Decision Process (MDP)"},{"location":"rl/#state-action-value-function-q-function","text":"This is expressed as the function $Q(s,a) = Return$ if: * You start in state $s$ * Take action $a$ (once) * Then behave optimally after that","title":"State Action Value Function (Q Function)"},{"location":"rl/#bellman-equation","text":"This equation is used to compute the Q Function. Take note of the following notation convention: Terminal states have the following Bellman Equation since they have no \"next\" state and action: $Q(s,a) = R(s)$ The Bellman Equation can be split into 2 components:","title":"Bellman Equation"},{"location":"rl/#stochastic-environment","text":"The system may not have deterministic / fixed actions. Eg. The Mars Rover could slip and make a step in the opposite direction. This warrants the introduction of randomness into the system. Return is treated as a random variable. The goal of the Bellman Equation would thus be to maximise the Expected Return , which is analogous to the average of the random variable Return . The Bellman Equation will include the expected maximum return of the next state.","title":"Stochastic Environment"},{"location":"rl/#continuous-state-spaces","text":"The initial simplified lunar landing rover example uses discrete states where it could be in 1 of 6 different positions. Real-life RL applications could involve states that take on continuous values. For example a vehicle could require the following continuous states: X-position Y-position Orientation / Facing Speed in x-direction Speed in y-direction Turning speed These continuous states are represented using a vector. Note that this vector can also contain discrete values if needed.","title":"Continuous State Spaces"},{"location":"rl/#deep-rl","text":"Deep RL will be discussed using the example of the Lunar Lander. x and y refer to horizontal and vertical distances. $\\dot{x}$ and $\\dot{y}$ refer to their corresponding velocities. $\\theta$ refers to the lander's angle and $\\dot{\\theta}$ refers to the velocity of its tilting. $r$ and $l$ are binary states referring to the grounding of the right and left legs of the lander. Final layer with a single neuron is used to output $Q(s,a)$ which will also be referred to as $y$. $a$ is a component of vector $x$ that comprises booleans representing the 4 actions to take. Nothing = no thrusters; left = left thrusters; right = right thruster; main = main thruster General concept is to: Use Bellman's Equation to create a large dataset of multiple $\\vec{x}$ and $y$ Use supervised learning to learn a mapping from $\\vec{x}$ to $y$ The algorithm to do this is called the Deep Q Network (DQN) method: A neural network $Q$ is randomly initialised similar to what is normally done for weight matrices. This initialisation is a random guess of what the $Q$ function should be. Generate the replay buffer (10,000 training examples) by feeding random $(s,a)$ into the neural network - which outputs the $Q(s,a)$ function. Record the resulting states and rewards $(R(s),s')$. A new neural network $Q_{new}$ is trained to map current (state and action) $x$, to the Bellman Equation $y$. The $Q(s',a')$ component of the Bellman Equation is obtained by passing $x = (s',a')$ into the initial $Q from step 1 for all 4 possible values of $a'$. $Q_{new}$ replaces the old $Q$ function. Steps 1 to 4 are repeated several times. With every iteration, the neural network will become a better estimate for the $Q$ function. The DQN architecture can be improved by: Setting its inputs to solely contain the states $s$ without the actions $a$. Changing the final layer to comprise a neuron to predict each action such that all actions can be predicted simultaneously.","title":"Deep RL"},{"location":"rl/#epsilon-greedy-policy","text":"Used to pick actions while system is still in the process of learning. Comprises a greedy exploitation component that picks an action $a$ that maximises the $Q$ function. Comprises a second $\\epsilon$ exploration hyperparameter to randomly try another action. This is to overcome the problem of potentially having a bad initialisation of $Q$ as exploration enables the system to try an action that it may have be initialised to recognise to result in a poor reward. Take note that RL is very sensitive to hyperparameters, which makes optimisation more difficult than supervised learning.","title":"Epsilon Greedy Policy"},{"location":"rl/#dqn-mini-batch-and-soft-updates","text":"Mini-batching results in a more efficient optmisation step. For example SGD using all samples would take more time as it requires computing the gradient of the loss function which is an average. However this may abruptly compromise the update of $Q$ to $Q_{new}$ if the mini-batch happens to produce a bad update Soft update addresses this problem by assigning hyperparameterised coefficients to weight the amount of update to the old neural network weights and biases.","title":"DQN Mini-Batch and Soft Updates"},{"location":"transformers/","text":"Transformers The transformer is an architecture built on the self-attention mechanism that was introduced in the seminal paper Attention is all you need (Vaswani, et al., 2017) as a simpler alternative to complex recurrent and convolutional structures for sequence transduction. The proposed transformer comprised a series of encoder and decoder stacks, where the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\textbf{z} = (z_1, ..., z_n)$. Given $\\textbf{z}$, the decoder then generates an output sequence $(y_1, ..., y_m)$ of symbols one element at a time. Other references: Illustrated transformer article Encoders Decoders In the Decoder-only architecture, the model consists of only a decoder, which is trained to predict the next token in a sequence given the previous tokens. The critical difference between the Decoder-only architecture and the Encoder-Decoder architecture is that the Decoder-only architecture does not have an explicit encoder to summarize the input information. Instead, the information is encoded implicitly in the hidden state of the decoder, which is updated at each step of the generation process. Applications Text completion Text generation Translation Question-Answering Generating image captions Decoding methods Greedy Search Greedy search simply selects the word with the highest probability as its next word: $w_t = argmax_w P(w\u2223w_{1:t\u22121})$ at each timestep t. Cons: * Greedy search tends to have repetitive outputs. * Greedy search misses high probability words hidden behind a low probability word: even though \"has\" has the highest conditional probability of 0.9, it comes after \"dog\" which has a smaller conditional probability than \"nice\" and is missed by the greedy search. Beam Search Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. The output still includes repetitions of the same word sequences. A simple remedy is to introduce n-grams (a.k.a word sequences of n words) penalties as introduced by Paulus et al. (2017) - specifically an intra-temporal attention function is used to penalise input tokens that have obtained high attention scores in past decoding steps. Cons: * Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output. * Repetitions in the output * Beam search works well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarisation. But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation. * High quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. Sampling Refers to randomly picking the next word $w_t$ according to its conditional probability distribution $w_t \\sim P(w | w_{1:t-1})$ The models often generate incoherent gibberish, cf. Ari Holtzman et al. (2019). A trick is to make the distribution $P(w | w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the \"temperature\" of the softmax. Cons: * While applying temperature can make a distribution less random, in its limit, when setting temperature $\\rightarrow$ 0, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before. Top-K Sampling Fan et. al (2018) introduced a simple, but very powerful sampling scheme, called Top-K sampling. In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation. As an example we look at 2 successive sampling steps where number of sampled words = 10, and K=6. We see that it successfully eliminates the rather weird candidates (\u201cnot\", \u201cthe\", \u201csmall\", \u201ctold\") in the second sampling step. Cons: * It does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w | w_{1:t-1})$. This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above). In step t=1, Top-K eliminates the possibility to sample (\"people\",\"big\",\"house\",\"cat\"), which seem like reasonable candidates. On the other hand, in step t=2 the method includes the arguably ill-fitted words (\"down\",\"a\") in the sample pool of words. Thus, limiting the sample pool to a fixed size K could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. This intuition led Ari Holtzman et al. (2019) to create Top-p or nucleus-sampling. Top-p (nucleus) sampling Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Contrastive Search Given the prefix text $x_{<t}$, the selection of the output token $x_t$ follows: where $V^{(k)}$ is the set of top-k predictions from the language model's probability distribution $p_{\\theta}(v | x_{<t})$. The first term, i.e. model confidence, is the probability of the candidate v predicted by the language model. The second term, degeneration penalty, measures how discriminative of v with respect to the previous context $x_{<t}$ and the function $s(\\cdot, \\cdot)$ computes the cosine similarity between the token representations. More specifically, the degeneration penalty is defined as the maximum cosine similarity between the token representation of v, ie. $h_v$, and that of all tokens in the context $x_{<t}$. Here, the candidate representation $h_v$ is computed by the language model given the concatenation of $x_{<t}$ and v. Intuitively, a larger degeneration penalty of v means it is more similar (in the representation space) to the context, therefore more likely leading to the problem of model degeneration. The hyperparameter $\\alpha$ regulates the importance of these two components. When $\\alpha$ = 0, contrastive search degenerates to the vanilla greedy search. Papers: * A deep reinforced model for abstractive summarization (Paulus, et al., 2017) Other references: * How to generate text (article)[https://huggingface.co/blog/how-to-generate] * Generating human-level text with contrasstive search in transformers (article)[https://huggingface.co/blog/introducing-csearch]","title":"Transformers"},{"location":"transformers/#transformers","text":"The transformer is an architecture built on the self-attention mechanism that was introduced in the seminal paper Attention is all you need (Vaswani, et al., 2017) as a simpler alternative to complex recurrent and convolutional structures for sequence transduction. The proposed transformer comprised a series of encoder and decoder stacks, where the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\textbf{z} = (z_1, ..., z_n)$. Given $\\textbf{z}$, the decoder then generates an output sequence $(y_1, ..., y_m)$ of symbols one element at a time. Other references: Illustrated transformer article","title":"Transformers"},{"location":"transformers/#encoders","text":"","title":"Encoders"},{"location":"transformers/#decoders","text":"In the Decoder-only architecture, the model consists of only a decoder, which is trained to predict the next token in a sequence given the previous tokens. The critical difference between the Decoder-only architecture and the Encoder-Decoder architecture is that the Decoder-only architecture does not have an explicit encoder to summarize the input information. Instead, the information is encoded implicitly in the hidden state of the decoder, which is updated at each step of the generation process.","title":"Decoders"},{"location":"transformers/#applications","text":"Text completion Text generation Translation Question-Answering Generating image captions","title":"Applications"},{"location":"transformers/#decoding-methods","text":"Greedy Search Greedy search simply selects the word with the highest probability as its next word: $w_t = argmax_w P(w\u2223w_{1:t\u22121})$ at each timestep t. Cons: * Greedy search tends to have repetitive outputs. * Greedy search misses high probability words hidden behind a low probability word: even though \"has\" has the highest conditional probability of 0.9, it comes after \"dog\" which has a smaller conditional probability than \"nice\" and is missed by the greedy search. Beam Search Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. The output still includes repetitions of the same word sequences. A simple remedy is to introduce n-grams (a.k.a word sequences of n words) penalties as introduced by Paulus et al. (2017) - specifically an intra-temporal attention function is used to penalise input tokens that have obtained high attention scores in past decoding steps. Cons: * Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output. * Repetitions in the output * Beam search works well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarisation. But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation. * High quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. Sampling Refers to randomly picking the next word $w_t$ according to its conditional probability distribution $w_t \\sim P(w | w_{1:t-1})$ The models often generate incoherent gibberish, cf. Ari Holtzman et al. (2019). A trick is to make the distribution $P(w | w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the \"temperature\" of the softmax. Cons: * While applying temperature can make a distribution less random, in its limit, when setting temperature $\\rightarrow$ 0, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before. Top-K Sampling Fan et. al (2018) introduced a simple, but very powerful sampling scheme, called Top-K sampling. In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation. As an example we look at 2 successive sampling steps where number of sampled words = 10, and K=6. We see that it successfully eliminates the rather weird candidates (\u201cnot\", \u201cthe\", \u201csmall\", \u201ctold\") in the second sampling step. Cons: * It does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w | w_{1:t-1})$. This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above). In step t=1, Top-K eliminates the possibility to sample (\"people\",\"big\",\"house\",\"cat\"), which seem like reasonable candidates. On the other hand, in step t=2 the method includes the arguably ill-fitted words (\"down\",\"a\") in the sample pool of words. Thus, limiting the sample pool to a fixed size K could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. This intuition led Ari Holtzman et al. (2019) to create Top-p or nucleus-sampling. Top-p (nucleus) sampling Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Contrastive Search Given the prefix text $x_{<t}$, the selection of the output token $x_t$ follows: where $V^{(k)}$ is the set of top-k predictions from the language model's probability distribution $p_{\\theta}(v | x_{<t})$. The first term, i.e. model confidence, is the probability of the candidate v predicted by the language model. The second term, degeneration penalty, measures how discriminative of v with respect to the previous context $x_{<t}$ and the function $s(\\cdot, \\cdot)$ computes the cosine similarity between the token representations. More specifically, the degeneration penalty is defined as the maximum cosine similarity between the token representation of v, ie. $h_v$, and that of all tokens in the context $x_{<t}$. Here, the candidate representation $h_v$ is computed by the language model given the concatenation of $x_{<t}$ and v. Intuitively, a larger degeneration penalty of v means it is more similar (in the representation space) to the context, therefore more likely leading to the problem of model degeneration. The hyperparameter $\\alpha$ regulates the importance of these two components. When $\\alpha$ = 0, contrastive search degenerates to the vanilla greedy search. Papers: * A deep reinforced model for abstractive summarization (Paulus, et al., 2017) Other references: * How to generate text (article)[https://huggingface.co/blog/how-to-generate] * Generating human-level text with contrasstive search in transformers (article)[https://huggingface.co/blog/introducing-csearch]","title":"Decoding methods"},{"location":"trees/","text":"Tree-Based Models Decision Trees The algorithm iteratively goes through each feature and finds the best split for each depth based on a splitting criterion. Classifier Trees Splitting criterion Gini Impurity For J classes where a i \u2208 {1, ..., J}: Entropy (Information Gain) Categorical feature splits This can be done in 1 of 2 ways: One-hot encoding: Every level has its own binary column. For a level x , a node will be split into child nodes of column x = 1 or 0. Ordinal encoding: There will be a single numerical column of integers representing the different levels. Considering how the input data matrix would not be sparse as with one-hot encoding, this may be computationally more efficient with the caveat that computing 1's and 0's are easier. Refer to how numeric splits work below. Numeric feature splits: Numeric column is first sorted from smallest to largest. Average is computed between each successive row. Each average is used to split the node and compute the split criterion metric. The average with the best split criterion metric is chosen. Prediction: If a leaf consists multiple samples, the predicted class is the mode. Regression Trees Splitting criterion: Sum of squared errors (SSR) Numeric feature splits: Iterate through each sample and use the average between the current and next sample as a threshold to split the dataset into 2 groups. Compute the combined SSR for both groups during each iteration. The average that results in the best split (lowest SSR) will be used to split the node. Categorical feature splits: Similar method to numeric feature splits is employed except that instead of the average between each sample being iteratively used as a threshold to split the dataset for SSR computation, the split used for SSR computation is based on whether samples have a categorical value or not. Prediction: If a leaf consists multiple samples, their average target value is taken to represent that leaf. Pruning Pre-pruning Post-pruning Random Forests AdaBoost The core principle of AdaBoost is to fit a sequence of weak learners on repeatedly modified versions of the data. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Training samples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, samples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the samples that are missed by the previous ones in the sequence. The key steps have been documented below: Initialise a sample weights column which indicates how important it is for a sample to be correctly classified. This is done by taking 1/m where m is the number of samples in the dataset - total weights should sum to 1. Build a stump with a relevant split criterion - Gini / Entropy for classification; SSR for regression. The stump only has 1 root node that splits into 2 leaves immediately. This is done using the feature that gives the best split criterion value. Compute amount of say (output weight) for the stump using total error [0,1], which is the sum of weights of the incorrectly classified samples: If stump total error is high, amount of say is negative - will reverse its output. Update weight samples such that incorrectly classified samples have greater weight values. Update weight samples such that correctly classified samples have smaller weight values. Normalise the updated weights so they sum to 1. Create the next stump either by: Using the normalised weights to compute the weighted split criterion per feature. Using the normalised weights as a distribution to draw samples from the original dataset. As incorrectly classified samples have large weights, they take up a larger range in the distribution and have a higher chance of being sampled. Sample until you get the same amount of rows as the original dataset. Lastly, reinitialise the sample weights as 1/m . The incorrectly classified samples need not retain their higher weight values as they should have been repeatedly sampled into duplicate rows. Prediction: Classification: The class predicted by the stumps with the highest sum of amount of say will be the final prediction Regression: The predictions from all stumps are then combined through a weighted sum to produce the final prediction (sklearn). Gradient Boost Make initial prediction: Regression: Average target value Classification: Initial prediction is a log odds where odds equal to class count / not class count . Weak learners are trees: That are split based on the standard decision tree splitting criterion. With 8 - 32 leaves. Scaled by the same learning rate. Trained to predict the pseudo residual. The pseudo residual: To compute the pseudo residual, the inital prediction value needs to be converted from a log odds into a probability using a logistic function: The residuals are then transformed back into a log odds output using this formula so that they can be added to the initial prediction which is a log odds : Can also be mathematically expressed as the function of the partial derivative / gradient of the loss function against the predicted value, hence the \"Gradient\" in \"Gradient Boost\": Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Prediction: For regression, this is done by adding scaled (by learning rate) predicted residuals of each learner to the initial prediction. For classification: Residuals have been transformed to a log odds output. This output is added to the initial prediction to gve a log odds prediction. An additional transformation needs to be applied to log odds to convert it into a probability using a logistic function: XGBoost Make initial prediction with value of 0.5 whether XGBoost is used for regression or classification. For classification, this is treated to be a probability . This is to be distinguished from standard Gradient Boost where the initial prediction is a log odds . Weak learners: Starts off as a root node of residuals (target - 0.5). Are NOT decision trees as they are split based on the gain in a similarity score (not split criterion such as Gini, Entropy, SSR) when comparing a parent node with its potential child nodes. This distinguishes XGBoost learners from Gradient Boost learners. The similarity score is a function of each sample's feature 'residual': eg. row 1 has col 1 value of 123 which gives residual of 23 from average prediction of 100. Similarity score for regression: Similarity score for classification which transforms residuals from a probability into a log odds . Its numerator is the same as the similarity score formula for regression.: The similarity gain is the difference between the similarity score of the parent and its potential child nodes. This is unlike standard Gradient Boost which grows its learners using a standard decision tree's splitting criterion. Prediction: Regression: First uses the following output formula to compute the value of each leaf. This is similar to the similarity score except that the sum of residuals numerator is not squared: The output scores are multiplied with the learning rate \u03b7 (eta) - with default value of 0.3 - and added to the initial predicted value: The new residuals are computed by taking target - (init pred + \u03b7 * output) . These residuals are then used to build the next tree. Classification: Similar to regression, the residuals are used to compute output values, albeit with a different formula. Again, the output formula is the same as the similarity score formula except that its numerator is not squared. The XGBoost output formula is the same as what was used in standard Gradient Boost to convert residuals into log odds except for the addition of Lambda in the denominator. To compute the new residuals, the initial prediction needs to be converted from a probability into a log odds using the formula below: The log odds predicted value is done using init pred + \u03b7 * output . It is then converted into a probability, pred proba . The new residuals are computed by taking target - pred proba , where target is 1 or 0. These residuals are then used to build the next tree. Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Hyperparameters: \u03bb (Lambda): L2 regularisation term Reduces prediction sensitivity to inidivual observations. More If Lambda > 0, similarity gain values are smaller. In conjunction with Gamma, pruning is more aggressive. \u03b1 (Alpha): L1 regularisation term \u03b3 (Gamma): Minimum similarity score gain required to make a further partition on a leaf node of the tree. Cover: Number of residuals in each leaf. This is equivalent to the denominator of the similarity score less Lambda. Summary Algorithm Learner Splits Bootstrap Learner Weights Predicts Ensemble Method Bias / Variance Random Forest Full trees Same as DT Random Equal Output Bagging High bias AdaBoost Stumps Same as DT None or weight-based Weighted Output Boosting High variance Gradient Boost Trees (8 - 32 leaves) Same as DT None Equal using learning rate Error / Pseudo residual Gradient Boosting High variance XGBoost \"XGBoost\" Trees (depth of 6) Similarity Score Gain None Equal using learning rate Error / Pseudo residual XGradient Boosting High variance Another important distinction between Gradient Boost and XGBoost is that their classififiers have different types of initial prediction values. Gradient Boost: Log odds XGBoost: Probability Algorithm Initial Prediction Residual Learner Prediction Gradient Boost Log odds Proba -> Log odds Log odds -> Proba XG Boost Proba -> Log odds Proba -> Log odds Log odds -> Proba","title":"Tree-Based Models"},{"location":"trees/#tree-based-models","text":"","title":"Tree-Based Models"},{"location":"trees/#decision-trees","text":"The algorithm iteratively goes through each feature and finds the best split for each depth based on a splitting criterion.","title":"Decision Trees"},{"location":"trees/#classifier-trees","text":"Splitting criterion Gini Impurity For J classes where a i \u2208 {1, ..., J}: Entropy (Information Gain) Categorical feature splits This can be done in 1 of 2 ways: One-hot encoding: Every level has its own binary column. For a level x , a node will be split into child nodes of column x = 1 or 0. Ordinal encoding: There will be a single numerical column of integers representing the different levels. Considering how the input data matrix would not be sparse as with one-hot encoding, this may be computationally more efficient with the caveat that computing 1's and 0's are easier. Refer to how numeric splits work below. Numeric feature splits: Numeric column is first sorted from smallest to largest. Average is computed between each successive row. Each average is used to split the node and compute the split criterion metric. The average with the best split criterion metric is chosen. Prediction: If a leaf consists multiple samples, the predicted class is the mode.","title":"Classifier Trees"},{"location":"trees/#regression-trees","text":"Splitting criterion: Sum of squared errors (SSR) Numeric feature splits: Iterate through each sample and use the average between the current and next sample as a threshold to split the dataset into 2 groups. Compute the combined SSR for both groups during each iteration. The average that results in the best split (lowest SSR) will be used to split the node. Categorical feature splits: Similar method to numeric feature splits is employed except that instead of the average between each sample being iteratively used as a threshold to split the dataset for SSR computation, the split used for SSR computation is based on whether samples have a categorical value or not. Prediction: If a leaf consists multiple samples, their average target value is taken to represent that leaf.","title":"Regression Trees"},{"location":"trees/#pruning","text":"Pre-pruning Post-pruning","title":"Pruning"},{"location":"trees/#random-forests","text":"","title":"Random Forests"},{"location":"trees/#adaboost","text":"The core principle of AdaBoost is to fit a sequence of weak learners on repeatedly modified versions of the data. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Training samples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, samples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the samples that are missed by the previous ones in the sequence. The key steps have been documented below: Initialise a sample weights column which indicates how important it is for a sample to be correctly classified. This is done by taking 1/m where m is the number of samples in the dataset - total weights should sum to 1. Build a stump with a relevant split criterion - Gini / Entropy for classification; SSR for regression. The stump only has 1 root node that splits into 2 leaves immediately. This is done using the feature that gives the best split criterion value. Compute amount of say (output weight) for the stump using total error [0,1], which is the sum of weights of the incorrectly classified samples: If stump total error is high, amount of say is negative - will reverse its output. Update weight samples such that incorrectly classified samples have greater weight values. Update weight samples such that correctly classified samples have smaller weight values. Normalise the updated weights so they sum to 1. Create the next stump either by: Using the normalised weights to compute the weighted split criterion per feature. Using the normalised weights as a distribution to draw samples from the original dataset. As incorrectly classified samples have large weights, they take up a larger range in the distribution and have a higher chance of being sampled. Sample until you get the same amount of rows as the original dataset. Lastly, reinitialise the sample weights as 1/m . The incorrectly classified samples need not retain their higher weight values as they should have been repeatedly sampled into duplicate rows. Prediction: Classification: The class predicted by the stumps with the highest sum of amount of say will be the final prediction Regression: The predictions from all stumps are then combined through a weighted sum to produce the final prediction (sklearn).","title":"AdaBoost"},{"location":"trees/#gradient-boost","text":"Make initial prediction: Regression: Average target value Classification: Initial prediction is a log odds where odds equal to class count / not class count . Weak learners are trees: That are split based on the standard decision tree splitting criterion. With 8 - 32 leaves. Scaled by the same learning rate. Trained to predict the pseudo residual. The pseudo residual: To compute the pseudo residual, the inital prediction value needs to be converted from a log odds into a probability using a logistic function: The residuals are then transformed back into a log odds output using this formula so that they can be added to the initial prediction which is a log odds : Can also be mathematically expressed as the function of the partial derivative / gradient of the loss function against the predicted value, hence the \"Gradient\" in \"Gradient Boost\": Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Prediction: For regression, this is done by adding scaled (by learning rate) predicted residuals of each learner to the initial prediction. For classification: Residuals have been transformed to a log odds output. This output is added to the initial prediction to gve a log odds prediction. An additional transformation needs to be applied to log odds to convert it into a probability using a logistic function:","title":"Gradient Boost"},{"location":"trees/#xgboost","text":"Make initial prediction with value of 0.5 whether XGBoost is used for regression or classification. For classification, this is treated to be a probability . This is to be distinguished from standard Gradient Boost where the initial prediction is a log odds . Weak learners: Starts off as a root node of residuals (target - 0.5). Are NOT decision trees as they are split based on the gain in a similarity score (not split criterion such as Gini, Entropy, SSR) when comparing a parent node with its potential child nodes. This distinguishes XGBoost learners from Gradient Boost learners. The similarity score is a function of each sample's feature 'residual': eg. row 1 has col 1 value of 123 which gives residual of 23 from average prediction of 100. Similarity score for regression: Similarity score for classification which transforms residuals from a probability into a log odds . Its numerator is the same as the similarity score formula for regression.: The similarity gain is the difference between the similarity score of the parent and its potential child nodes. This is unlike standard Gradient Boost which grows its learners using a standard decision tree's splitting criterion. Prediction: Regression: First uses the following output formula to compute the value of each leaf. This is similar to the similarity score except that the sum of residuals numerator is not squared: The output scores are multiplied with the learning rate \u03b7 (eta) - with default value of 0.3 - and added to the initial predicted value: The new residuals are computed by taking target - (init pred + \u03b7 * output) . These residuals are then used to build the next tree. Classification: Similar to regression, the residuals are used to compute output values, albeit with a different formula. Again, the output formula is the same as the similarity score formula except that its numerator is not squared. The XGBoost output formula is the same as what was used in standard Gradient Boost to convert residuals into log odds except for the addition of Lambda in the denominator. To compute the new residuals, the initial prediction needs to be converted from a probability into a log odds using the formula below: The log odds predicted value is done using init pred + \u03b7 * output . It is then converted into a probability, pred proba . The new residuals are computed by taking target - pred proba , where target is 1 or 0. These residuals are then used to build the next tree. Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Hyperparameters: \u03bb (Lambda): L2 regularisation term Reduces prediction sensitivity to inidivual observations. More If Lambda > 0, similarity gain values are smaller. In conjunction with Gamma, pruning is more aggressive. \u03b1 (Alpha): L1 regularisation term \u03b3 (Gamma): Minimum similarity score gain required to make a further partition on a leaf node of the tree. Cover: Number of residuals in each leaf. This is equivalent to the denominator of the similarity score less Lambda.","title":"XGBoost"},{"location":"trees/#summary","text":"Algorithm Learner Splits Bootstrap Learner Weights Predicts Ensemble Method Bias / Variance Random Forest Full trees Same as DT Random Equal Output Bagging High bias AdaBoost Stumps Same as DT None or weight-based Weighted Output Boosting High variance Gradient Boost Trees (8 - 32 leaves) Same as DT None Equal using learning rate Error / Pseudo residual Gradient Boosting High variance XGBoost \"XGBoost\" Trees (depth of 6) Similarity Score Gain None Equal using learning rate Error / Pseudo residual XGradient Boosting High variance Another important distinction between Gradient Boost and XGBoost is that their classififiers have different types of initial prediction values. Gradient Boost: Log odds XGBoost: Probability Algorithm Initial Prediction Residual Learner Prediction Gradient Boost Log odds Proba -> Log odds Log odds -> Proba XG Boost Proba -> Log odds Proba -> Log odds Log odds -> Proba","title":"Summary"}]}