{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Matt's AI Notes This archive of notes was created using mkdocs. For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Welcome to Matt's AI Notes"},{"location":"#welcome-to-matts-ai-notes","text":"This archive of notes was created using mkdocs. For full documentation visit mkdocs.org .","title":"Welcome to Matt's AI Notes"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"hyperparam/","text":"Hyperparameter Tuning Nested Cross-Validation Bayesian Tree-structured Parzen Estimator (TPE)","title":"Hyperparameter Tuning"},{"location":"hyperparam/#hyperparameter-tuning","text":"","title":"Hyperparameter Tuning"},{"location":"hyperparam/#nested-cross-validation","text":"","title":"Nested Cross-Validation"},{"location":"hyperparam/#bayesian","text":"","title":"Bayesian"},{"location":"hyperparam/#tree-structured-parzen-estimator-tpe","text":"","title":"Tree-structured Parzen Estimator (TPE)"},{"location":"trees/","text":"Tree-Based Models Decision Trees The algorithm iteratively goes through each feature and finds the best split for each depth based on a splitting criterion. Classifier Trees Splitting criterion Gini Impurity For J classes where a i \u2208 {1, ..., J}: Entropy (Information Gain) Categorical feature splits This can be done in 1 of 2 ways: One-hot encoding: Every level has its own binary column. For a level x , a node will be split into child nodes of column x = 1 or 0. Ordinal encoding: There will be a single numerical column of integers representing the different levels. Considering how the input data matrix would not be sparse as with one-hot encoding, this may be computationally more efficient with the caveat that computing 1's and 0's are easier. Refer to how numeric splits work below. Numeric feature splits: Numeric column is first sorted from smallest to largest. Average is computed between each successive row. Each average is used to split the node and compute the split criterion metric. The average with the best split criterion metric is chosen. Prediction: If a leaf consists multiple samples, the predicted class is the mode. Regression Trees Splitting criterion: Sum of squared errors (SSR) Numeric feature splits: Iterate through each sample and use the average between the current and next sample as a threshold to split the dataset into 2 groups. Compute the combined SSR for both groups during each iteration. The average that results in the best split (lowest SSR) will be used to split the node. Categorical feature splits: Similar method to numeric feature splits is employed except that instead of the average between each sample being iteratively used as a threshold to split the dataset for SSR computation, the split used for SSR computation is based on whether samples have a categorical value or not. Prediction: If a leaf consists multiple samples, their average target value is taken to represent that leaf. Pruning Pre-pruning Post-pruning Random Forests AdaBoost The core principle of AdaBoost is to fit a sequence of weak learners on repeatedly modified versions of the data. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Training samples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, samples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the samples that are missed by the previous ones in the sequence. The key steps have been documented below: Initialise a sample weights column which indicates how important it is for a sample to be correctly classified. This is done by taking 1/m where m is the number of samples in the dataset - total weights should sum to 1. Build a stump with a relevant split criterion - Gini / Entropy for classification; SSR for regression. The stump only has 1 root node that splits into 2 leaves immediately. This is done using the feature that gives the best split criterion value. Compute amount of say (output weight) for the stump using total error [0,1], which is the sum of weights of the incorrectly classified samples: If stump total error is high, amount of say is negative - will reverse its output. Update weight samples such that incorrectly classified samples have greater weight values. Update weight samples such that correctly classified samples have smaller weight values. Normalise the updated weights so they sum to 1. Create the next stump either by: Using the normalised weights to compute the weighted split criterion per feature. Using the normalised weights as a distribution to draw samples from the original dataset. As incorrectly classified samples have large weights, they take up a larger range in the distribution and have a higher chance of being sampled. Sample until you get the same amount of rows as the original dataset. Lastly, reinitialise the sample weights as 1/m . The incorrectly classified samples need not retain their higher weight values as they should have been repeatedly sampled into duplicate rows. Prediction: Classification: The class predicted by the stumps with the highest sum of amount of say will be the final prediction Regression: The predictions from all stumps are then combined through a weighted sum to produce the final prediction (sklearn). Gradient Boost Make initial prediction: Regression: Average target value Classification: Initial prediction is a log odds where odds equal to class count / not class count . Weak learners are trees: That are split based on the standard decision tree splitting criterion. With 8 - 32 leaves. Scaled by the same learning rate. Trained to predict the pseudo residual. The pseudo residual: To compute the pseudo residual, the inital prediction value needs to be converted from a log odds into a probability using a logistic function: The residuals are then transformed back into a log odds output using this formula so that they can be added to the initial prediction which is a log odds : Can also be mathematically expressed as the function of the partial derivative / gradient of the loss function against the predicted value, hence the \"Gradient\" in \"Gradient Boost\": Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Prediction: For regression, this is done by adding scaled (by learning rate) predicted residuals of each learner to the initial prediction. For classification: Residuals have been transformed to a log odds output. This output is added to the initial prediction to gve a log odds prediction. An additional transformation needs to be applied to log odds to convert it into a probability using a logistic function: XGBoost Make initial prediction with value of 0.5 whether XGBoost is used for regression or classification. For classification, this is treated to be a probability . This is to be distinguished from standard Gradient Boost where the initial prediction is a log odds . Weak learners: Starts off as a root node of residuals (target - 0.5). Are NOT decision trees as they are split based on the gain in a similarity score (not split criterion such as Gini, Entropy, SSR) when comparing a parent node with its potential child nodes. This distinguishes XGBoost learners from Gradient Boost learners. The similarity score is a function of each sample's feature 'residual': eg. row 1 has col 1 value of 123 which gives residual of 23 from average prediction of 100. Similarity score for regression: Similarity score for classification which transforms residuals from a probability into a log odds . Its numerator is the same as the similarity score formula for regression.: The similarity gain is the difference between the similarity score of the parent and its potential child nodes. This is unlike standard Gradient Boost which grows its learners using a standard decision tree's splitting criterion. Prediction: Regression: First uses the following output formula to compute the value of each leaf. This is similar to the similarity score except that the sum of residuals numerator is not squared: The output scores are multiplied with the learning rate \u03b7 (eta) - with default value of 0.3 - and added to the initial predicted value: The new residuals are computed by taking target - (init pred + \u03b7 * output) . These residuals are then used to build the next tree. Classification: Similar to regression, the residuals are used to compute output values, albeit with a different formula. Again, the output formula is the same as the similarity score formula except that its numerator is not squared. The XGBoost output formula is the same as what was used in standard Gradient Boost to convert residuals into log odds except for the addition of Lambda in the denominator. To compute the new residuals, the initial prediction needs to be converted from a probability into a log odds using the formula below: The log odds predicted value is done using init pred + \u03b7 * output . It is then converted into a probability, pred proba . The new residuals are computed by taking target - pred proba , where target is 1 or 0. These residuals are then used to build the next tree. Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Hyperparameters: \u03bb (Lambda): L2 regularisation term Reduces prediction sensitivity to inidivual observations. More If Lambda > 0, similarity gain values are smaller. In conjunction with Gamma, pruning is more aggressive. \u03b1 (Alpha): L1 regularisation term \u03b3 (Gamma): Minimum similarity score gain required to make a further partition on a leaf node of the tree. Cover: Number of residuals in each leaf. This is equivalent to the denominator of the similarity score less Lambda. Summary Algorithm Learner Splits Bootstrap Learner Weights Predicts Ensemble Method Bias / Variance Random Forest Full trees Same as DT Random Equal Output Bagging High bias AdaBoost Stumps Same as DT None or weight-based Weighted Output Boosting High variance Gradient Boost Trees (8 - 32 leaves) Same as DT None Equal using learning rate Error / Pseudo residual Gradient Boosting High variance XGBoost \"XGBoost\" Trees (depth of 6) Similarity Score Gain None Equal using learning rate Error / Pseudo residual XGradient Boosting High variance Another important distinction between Gradient Boost and XGBoost is that their classififiers have different types of initial prediction values. Gradient Boost: Log odds XGBoost: Probability Algorithm Initial Prediction Residual Learner Prediction Gradient Boost Log odds Proba -> Log odds Log odds -> Proba XG Boost Proba -> Log odds Proba -> Log odds Log odds -> Proba","title":"Tree-Based Models"},{"location":"trees/#tree-based-models","text":"","title":"Tree-Based Models"},{"location":"trees/#decision-trees","text":"The algorithm iteratively goes through each feature and finds the best split for each depth based on a splitting criterion.","title":"Decision Trees"},{"location":"trees/#classifier-trees","text":"Splitting criterion Gini Impurity For J classes where a i \u2208 {1, ..., J}: Entropy (Information Gain) Categorical feature splits This can be done in 1 of 2 ways: One-hot encoding: Every level has its own binary column. For a level x , a node will be split into child nodes of column x = 1 or 0. Ordinal encoding: There will be a single numerical column of integers representing the different levels. Considering how the input data matrix would not be sparse as with one-hot encoding, this may be computationally more efficient with the caveat that computing 1's and 0's are easier. Refer to how numeric splits work below. Numeric feature splits: Numeric column is first sorted from smallest to largest. Average is computed between each successive row. Each average is used to split the node and compute the split criterion metric. The average with the best split criterion metric is chosen. Prediction: If a leaf consists multiple samples, the predicted class is the mode.","title":"Classifier Trees"},{"location":"trees/#regression-trees","text":"Splitting criterion: Sum of squared errors (SSR) Numeric feature splits: Iterate through each sample and use the average between the current and next sample as a threshold to split the dataset into 2 groups. Compute the combined SSR for both groups during each iteration. The average that results in the best split (lowest SSR) will be used to split the node. Categorical feature splits: Similar method to numeric feature splits is employed except that instead of the average between each sample being iteratively used as a threshold to split the dataset for SSR computation, the split used for SSR computation is based on whether samples have a categorical value or not. Prediction: If a leaf consists multiple samples, their average target value is taken to represent that leaf.","title":"Regression Trees"},{"location":"trees/#pruning","text":"Pre-pruning Post-pruning","title":"Pruning"},{"location":"trees/#random-forests","text":"","title":"Random Forests"},{"location":"trees/#adaboost","text":"The core principle of AdaBoost is to fit a sequence of weak learners on repeatedly modified versions of the data. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Training samples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, samples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the samples that are missed by the previous ones in the sequence. The key steps have been documented below: Initialise a sample weights column which indicates how important it is for a sample to be correctly classified. This is done by taking 1/m where m is the number of samples in the dataset - total weights should sum to 1. Build a stump with a relevant split criterion - Gini / Entropy for classification; SSR for regression. The stump only has 1 root node that splits into 2 leaves immediately. This is done using the feature that gives the best split criterion value. Compute amount of say (output weight) for the stump using total error [0,1], which is the sum of weights of the incorrectly classified samples: If stump total error is high, amount of say is negative - will reverse its output. Update weight samples such that incorrectly classified samples have greater weight values. Update weight samples such that correctly classified samples have smaller weight values. Normalise the updated weights so they sum to 1. Create the next stump either by: Using the normalised weights to compute the weighted split criterion per feature. Using the normalised weights as a distribution to draw samples from the original dataset. As incorrectly classified samples have large weights, they take up a larger range in the distribution and have a higher chance of being sampled. Sample until you get the same amount of rows as the original dataset. Lastly, reinitialise the sample weights as 1/m . The incorrectly classified samples need not retain their higher weight values as they should have been repeatedly sampled into duplicate rows. Prediction: Classification: The class predicted by the stumps with the highest sum of amount of say will be the final prediction Regression: The predictions from all stumps are then combined through a weighted sum to produce the final prediction (sklearn).","title":"AdaBoost"},{"location":"trees/#gradient-boost","text":"Make initial prediction: Regression: Average target value Classification: Initial prediction is a log odds where odds equal to class count / not class count . Weak learners are trees: That are split based on the standard decision tree splitting criterion. With 8 - 32 leaves. Scaled by the same learning rate. Trained to predict the pseudo residual. The pseudo residual: To compute the pseudo residual, the inital prediction value needs to be converted from a log odds into a probability using a logistic function: The residuals are then transformed back into a log odds output using this formula so that they can be added to the initial prediction which is a log odds : Can also be mathematically expressed as the function of the partial derivative / gradient of the loss function against the predicted value, hence the \"Gradient\" in \"Gradient Boost\": Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Prediction: For regression, this is done by adding scaled (by learning rate) predicted residuals of each learner to the initial prediction. For classification: Residuals have been transformed to a log odds output. This output is added to the initial prediction to gve a log odds prediction. An additional transformation needs to be applied to log odds to convert it into a probability using a logistic function:","title":"Gradient Boost"},{"location":"trees/#xgboost","text":"Make initial prediction with value of 0.5 whether XGBoost is used for regression or classification. For classification, this is treated to be a probability . This is to be distinguished from standard Gradient Boost where the initial prediction is a log odds . Weak learners: Starts off as a root node of residuals (target - 0.5). Are NOT decision trees as they are split based on the gain in a similarity score (not split criterion such as Gini, Entropy, SSR) when comparing a parent node with its potential child nodes. This distinguishes XGBoost learners from Gradient Boost learners. The similarity score is a function of each sample's feature 'residual': eg. row 1 has col 1 value of 123 which gives residual of 23 from average prediction of 100. Similarity score for regression: Similarity score for classification which transforms residuals from a probability into a log odds . Its numerator is the same as the similarity score formula for regression.: The similarity gain is the difference between the similarity score of the parent and its potential child nodes. This is unlike standard Gradient Boost which grows its learners using a standard decision tree's splitting criterion. Prediction: Regression: First uses the following output formula to compute the value of each leaf. This is similar to the similarity score except that the sum of residuals numerator is not squared: The output scores are multiplied with the learning rate \u03b7 (eta) - with default value of 0.3 - and added to the initial predicted value: The new residuals are computed by taking target - (init pred + \u03b7 * output) . These residuals are then used to build the next tree. Classification: Similar to regression, the residuals are used to compute output values, albeit with a different formula. Again, the output formula is the same as the similarity score formula except that its numerator is not squared. The XGBoost output formula is the same as what was used in standard Gradient Boost to convert residuals into log odds except for the addition of Lambda in the denominator. To compute the new residuals, the initial prediction needs to be converted from a probability into a log odds using the formula below: The log odds predicted value is done using init pred + \u03b7 * output . It is then converted into a probability, pred proba . The new residuals are computed by taking target - pred proba , where target is 1 or 0. These residuals are then used to build the next tree. Keep building more trees until Additional trees do not significantly reduce size of residuals. Maximum number of trees built. Hyperparameters: \u03bb (Lambda): L2 regularisation term Reduces prediction sensitivity to inidivual observations. More If Lambda > 0, similarity gain values are smaller. In conjunction with Gamma, pruning is more aggressive. \u03b1 (Alpha): L1 regularisation term \u03b3 (Gamma): Minimum similarity score gain required to make a further partition on a leaf node of the tree. Cover: Number of residuals in each leaf. This is equivalent to the denominator of the similarity score less Lambda.","title":"XGBoost"},{"location":"trees/#summary","text":"Algorithm Learner Splits Bootstrap Learner Weights Predicts Ensemble Method Bias / Variance Random Forest Full trees Same as DT Random Equal Output Bagging High bias AdaBoost Stumps Same as DT None or weight-based Weighted Output Boosting High variance Gradient Boost Trees (8 - 32 leaves) Same as DT None Equal using learning rate Error / Pseudo residual Gradient Boosting High variance XGBoost \"XGBoost\" Trees (depth of 6) Similarity Score Gain None Equal using learning rate Error / Pseudo residual XGradient Boosting High variance Another important distinction between Gradient Boost and XGBoost is that their classififiers have different types of initial prediction values. Gradient Boost: Log odds XGBoost: Probability Algorithm Initial Prediction Residual Learner Prediction Gradient Boost Log odds Proba -> Log odds Log odds -> Proba XG Boost Proba -> Log odds Proba -> Log odds Log odds -> Proba","title":"Summary"}]}