{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Matt's AI Notes This archive of notes was created using mkdocs. For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Welcome to Matt's AI Notes"},{"location":"#welcome-to-matts-ai-notes","text":"This archive of notes was created using mkdocs. For full documentation visit mkdocs.org .","title":"Welcome to Matt's AI Notes"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"hyperparam/","text":"Hyperparameter Tunining Nested Cross-Validation Bayesian Tree-structured Parzen Estimator (TPE)","title":"Hyperparameter Tunining"},{"location":"hyperparam/#hyperparameter-tunining","text":"","title":"Hyperparameter Tunining"},{"location":"hyperparam/#nested-cross-validation","text":"","title":"Nested Cross-Validation"},{"location":"hyperparam/#bayesian","text":"","title":"Bayesian"},{"location":"hyperparam/#tree-structured-parzen-estimator-tpe","text":"","title":"Tree-structured Parzen Estimator (TPE)"},{"location":"trees/","text":"Tree-Based Models Decision Trees The algorithm iteratively goes through each feature and finds the best split for each depth based on a splitting criterion. Classifier Trees Splitting criterion Gini Impurity For J classes where a i \u2208 {1, ..., J}: Entropy (Information Gain) Categorical feature splits This can be done in 1 of 2 ways: One-hot encoding: Every level has its own binary column. For a level x , a node will be split into child nodes of column x = 1 or 0. Ordinal encoding: There will be a single numerical column of integers representing the different levels. Considering how the input data matrix would not be sparse as with one-hot encoding, this may be computationally more efficient with the caveat that computing 1's and 0's are easier. Refer to how numeric splits work below. Numeric feature splits: Numeric column is first sorted from smallest to largest. Average is computed between each successive row. Each average is used to split the node and compute the split criterion metric. The average with the best split criterion metric is chosen. Regression Trees Splitting criterion: Sum of squared errors (SSR) Numeric feature splits: Iterate through each sample and use the average between the current and next sample as a threshold to split the dataset into 2 groups. Compute the combined SSR for both groups during each iteration. The average that results in the best split (lowest SSR) will be used to split the node. Categorical feature splits: Similar method to numeric feature splits is employed except that instead of the average between each sample being iteratively used as a threshold to split the dataset for SSR computation, the split used for SSR computation is based on whether samples have a categorical value or not. Pruning Pre-pruning Post-pruning Random Forests Ada Boost Gradient Boost XG Boost","title":"Tree-Based Models"},{"location":"trees/#tree-based-models","text":"","title":"Tree-Based Models"},{"location":"trees/#decision-trees","text":"The algorithm iteratively goes through each feature and finds the best split for each depth based on a splitting criterion.","title":"Decision Trees"},{"location":"trees/#classifier-trees","text":"Splitting criterion Gini Impurity For J classes where a i \u2208 {1, ..., J}: Entropy (Information Gain) Categorical feature splits This can be done in 1 of 2 ways: One-hot encoding: Every level has its own binary column. For a level x , a node will be split into child nodes of column x = 1 or 0. Ordinal encoding: There will be a single numerical column of integers representing the different levels. Considering how the input data matrix would not be sparse as with one-hot encoding, this may be computationally more efficient with the caveat that computing 1's and 0's are easier. Refer to how numeric splits work below. Numeric feature splits: Numeric column is first sorted from smallest to largest. Average is computed between each successive row. Each average is used to split the node and compute the split criterion metric. The average with the best split criterion metric is chosen.","title":"Classifier Trees"},{"location":"trees/#regression-trees","text":"Splitting criterion: Sum of squared errors (SSR) Numeric feature splits: Iterate through each sample and use the average between the current and next sample as a threshold to split the dataset into 2 groups. Compute the combined SSR for both groups during each iteration. The average that results in the best split (lowest SSR) will be used to split the node. Categorical feature splits: Similar method to numeric feature splits is employed except that instead of the average between each sample being iteratively used as a threshold to split the dataset for SSR computation, the split used for SSR computation is based on whether samples have a categorical value or not.","title":"Regression Trees"},{"location":"trees/#pruning","text":"Pre-pruning Post-pruning","title":"Pruning"},{"location":"trees/#random-forests","text":"","title":"Random Forests"},{"location":"trees/#ada-boost","text":"","title":"Ada Boost"},{"location":"trees/#gradient-boost","text":"","title":"Gradient Boost"},{"location":"trees/#xg-boost","text":"","title":"XG Boost"}]}