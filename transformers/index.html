<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://mattoh91.github.io/ml-notes/transformers/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Transformers - Matt's ML Notes</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Transformers";
        var mkdocs_page_input_path = "transformers.md";
        var mkdocs_page_url = "/ml-notes/transformers/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Matt's ML Notes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Welcome to Matt's AI Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../attention/">Attention</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../hyperparam/">Hyperparameter Tuning</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../llms/">BERT</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nlp/">NLP</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../regularisation/">Regularisation</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Transformers</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../trees/">Tree-Based Models</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Matt's ML Notes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>Transformers</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="transformers">Transformers</h1>
<p>The transformer is an architecture built on the self-attention mechanism that was introduced in the seminal paper <a href="https://arxiv.org/abs/1706.03762">Attention is all you need (Vaswani, et al., 2017)</a> as a simpler alternative to complex recurrent and convolutional structures for sequence transduction.</p>
<p><img alt="transformer architecture" src="../img/transformer_archi.png" /></p>
<p>The proposed transformer comprised a series of encoder and decoder stacks, where the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\textbf{z} = (z_1, ..., z_n)$. Given $\textbf{z}$, the decoder then generates an output sequence $(y_1, ..., y_m)$ of symbols one element at a time. </p>
<ul>
<li>Other references:</li>
<li>Illustrated transformer <a href="https://jalammar.github.io/illustrated-transformer/">article</a></li>
</ul>
<h1 id="encoders">Encoders</h1>
<h1 id="decoders">Decoders</h1>
<p>In the Decoder-only architecture, the model consists of only a decoder, which is trained to predict the next token in a sequence given the previous tokens. The critical difference between the Decoder-only architecture and the Encoder-Decoder architecture is that the Decoder-only architecture does not have an explicit encoder to summarize the input information. Instead, the information is encoded implicitly in the hidden state of the decoder, which is updated at each step of the generation process.</p>
<h2 id="applications">Applications</h2>
<ul>
<li>Text completion</li>
<li>Text generation</li>
<li>Translation</li>
<li>Question-Answering</li>
<li>Generating image captions</li>
</ul>
<h2 id="decoding-methods">Decoding methods</h2>
<ol>
<li>
<p>Greedy Search<br />
Greedy search simply selects the word with the highest probability as its next word: 
$w_t = argmax_w P(w∣w_{1:t−1})$ at each timestep t.</p>
<p><img alt="greedy search" src="../img/greedy_search.png" /></p>
</li>
</ol>
<p>Cons:
* Greedy search tends to have repetitive outputs.
* Greedy search misses high probability words hidden behind a low probability word: even though "has" has the highest conditional probability of 0.9, it comes after "dog" which has a smaller conditional probability than "nice" and is missed by the greedy search.</p>
<ol>
<li>
<p>Beam Search<br />
Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely <code>num_beams</code> of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. </p>
<p><img alt="beam search" src="../img/beam_search.png" /></p>
</li>
</ol>
<p>The output still includes repetitions of the same word sequences. A simple remedy is to introduce n-grams (a.k.a word sequences of n words) penalties as introduced by Paulus et al. (2017) - specifically an intra-temporal attention function is used to penalise input tokens that have obtained high attention scores in past decoding steps.</p>
<p>Cons:
* Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.
* Repetitions in the output
* Beam search works well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarisation. But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.
* High quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable.</p>
<ol>
<li>Sampling<br />
Refers to randomly picking the next word $w_t$ according to its conditional probability distribution $w_t \sim P(w | w_{1:t-1})$</li>
</ol>
<p>The models often generate incoherent gibberish, cf. Ari Holtzman et al. (2019).</p>
<p>A trick is to make the distribution $P(w | w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the "temperature" of the softmax.</p>
<p>Cons:
* While applying temperature can make a distribution less random, in its limit, when setting temperature $\rightarrow$ 0, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.</p>
<ol>
<li>
<p>Top-K Sampling<br />
Fan et. al (2018) introduced a simple, but very powerful sampling scheme, called Top-K sampling. In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation.<br />
As an example we look at 2 successive sampling steps where number of sampled words = 10, and K=6.</p>
<p><img alt="top-k sampling" src="../img/top_k_sampling.png" /></p>
</li>
</ol>
<p>We see that it successfully eliminates the rather weird candidates (“not", “the", “small", “told") in the second sampling step.</p>
<p>Cons:
* It does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w | w_{1:t-1})$. This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above). In step t=1, Top-K eliminates the possibility to sample ("people","big","house","cat"), which seem like reasonable candidates. On the other hand, in step t=2 the method includes the arguably ill-fitted words ("down","a") in the sample pool of words. Thus, limiting the sample pool to a fixed size K could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution. This intuition led Ari Holtzman et al. (2019) to create Top-p or nucleus-sampling.</p>
<ol>
<li>
<p>Top-p (nucleus) sampling<br />
Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. </p>
</li>
<li>
<p>Contrastive Search<br />
Given the prefix text $x_{&lt;t}$, the selection of the output token $x_t$ follows:</p>
<p><img alt="contrastive search" src="../img/csearch.png" /></p>
</li>
</ol>
<p>where $V^{(k)}$ is the set of top-k predictions from the language model's probability distribution $p_{\theta}(v | x_{&lt;t})$.<br />
The first term, i.e. model confidence, is the probability of the candidate v predicted by the language model.<br />
The second term, degeneration penalty, measures how discriminative of v with respect to the previous context $x_{&lt;t}$ and the function  $s(\cdot, \cdot)$ computes the cosine similarity between the token representations. More specifically, the degeneration penalty is defined as the maximum cosine similarity between the token representation of v, ie. $h_v$, and that of all tokens in the context $x_{&lt;t}$. Here, the candidate representation $h_v$ is computed by the language model given the concatenation of $x_{&lt;t}$ and v.<br />
Intuitively, a larger degeneration penalty of v means it is more similar (in the representation space) to the context, therefore more likely leading to the problem of model degeneration.<br />
The hyperparameter $\alpha$ regulates the importance of these two components. When $\alpha$ = 0, contrastive search degenerates to the vanilla greedy search.</p>
<p>Papers:
* <a href="https://arxiv.org/pdf/1705.04304.pdf">A deep reinforced model for abstractive summarization (Paulus, et al., 2017)</a>
Other references: 
* How to generate text (article)[https://huggingface.co/blog/how-to-generate]
* Generating human-level text with contrasstive search in transformers (article)[https://huggingface.co/blog/introducing-csearch]</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../regularisation/" class="btn btn-neutral float-left" title="Regularisation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../trees/" class="btn btn-neutral float-right" title="Tree-Based Models">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../regularisation/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../trees/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
